import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np

"""
PINN methods were taken from a simple model for an ODE in 'A Hands-on Introduction to Physics-Informed 
Neural Networks' on NanoHub by Atharva Hans and Ilias Bilionis, and applied to a more complex ODE and validated
with the finite difference method.

We will be solving a second order and non linear ODE:

d²Ψ/dx² = f(x, Ψ, Ψ'),   x ∈ [0,1],  
Ψ(0) = A,
Ψ'(0) = B,

where 

f(x, Ψ, Ψ') = dΨ/dx + Ψ² - x,

We will write the solution as the trained function

Ψ̂(x; θ) = A + B⋅x + N(x; θ)⋅x²,   

for a neural network, N(x; θ). The solution must satisfy d²Ψ/dx² - f(x, Ψ, Ψ') = 0, so 
we will define the loss as 

L(θ) = ∫_0^1 [d²Ψ̂(x;θ)/dx² − f(x,Ψ̂(x;θ), Ψ̂'(x;θ))]² dx,

which the neural network will be trained to minimize.
"""

# Hyperparameters -------------------------------------------------------------
A, B = -1.0, -5.0
L = 1
f = lambda x, Psi, Psi_x: Psi_x + Psi ** 2 - x

N_pts = 200
EPOCHS = 1000

torch.manual_seed(0)
np.random.seed(0)

x = np.linspace(0.0, L, N_pts, dtype=float)
Δx = x[1] - x[0]
x_torch = torch.from_numpy(x).float().view(-1, 1)  # from numpy to torch

Psi_trial = lambda x: A + B * x + x**2 * N(x)

# Physics Informed Neural Network -------------------------------------------
N = nn.Sequential(
    nn.Linear(1, 50), 
    nn.Sigmoid(), 
    nn.Linear(50,1, bias=False))

# Loss & Optimizer ------------------------------------------------------------
def total_loss(x):
    x.requires_grad = True
    Psi = Psi_trial(x)
    Psi_x = torch.autograd.grad(Psi, x, grad_outputs=torch.ones_like(Psi), create_graph=True)[0]
    Psi_xx = torch.autograd.grad(Psi_x, x, grad_outputs=torch.ones_like(Psi_x), create_graph=True)[0]
    return torch.mean((Psi_xx - f(x, Psi, Psi_x)) ** 2)

optimizer = torch.optim.LBFGS(N.parameters())

# Training Loop ---------------------------------------------------------------
def closure():  # must have a closure function written like this for LBFGS to work
    # 1. & 2. Forward Pass & Loss
    loss = total_loss(x_torch)  

    # 3. Optimizer Zero Grad
    optimizer.zero_grad()

    # 4. Backpropagation
    loss.backward()
    return loss

for epoch in range(EPOCHS):
    # 5. Gradient Descent
    optimizer.step(closure)

# Finite Difference Validation ----------------------------------------------
Psi_fd = np.zeros(N_pts, dtype=float)
Psi_fd[0] = A
Psi_fd[1] = A + B*Δx

for i in range(1, N_pts-1):
    Psi_fd_x = (Psi_fd[i] - Psi_fd[i-1]) / Δx 
    f_fd = Psi_fd_x + Psi_fd[i]**2 - x[i]

    Psi_fd[i+1] = (Δx**2) * f_fd + 2*Psi_fd[i] - Psi_fd[i-1]

# Plot & Error --------------------------------------------------------------
Psi_plot = Psi_trial(x_torch).detach().numpy().flatten()

plt.plot(x, Psi_fd, 'C1--', label='finite difference (marching)')
plt.plot(x, Psi_plot, label=r'$\Psi_t(x)$ (PINN Approximation)')
plt.legend(); plt.grid(True); plt.xlabel('x'); plt.ylabel(r'$\Psi$'); plt.show()

mse = np.mean((np.interp(x, x.squeeze(), Psi_plot) - Psi_fd)**2)
print("")
print(f'Mean Square Error (PINN vs FDM) = {mse:.3e}')
print("")